import os
from dotenv import load_dotenv
from supabase import create_client, Client
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. ‡πÇ‡∏´‡∏•‡∏î Config
load_dotenv()
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(url, key)

print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö... (‡πÇ‡∏´‡∏•‡∏î Embedding Model)")
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM (Typhoon ‡∏ú‡πà‡∏≤‡∏ô Ollama)
# ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ä‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô model ‡πÄ‡∏õ‡πá‡∏ô 'gemma2' ‡∏´‡∏£‡∏∑‡∏≠ 'llama3' ‡∏î‡∏π‡∏Ñ‡∏£‡∏±‡∏ö
llm = ChatOllama(model="llama3.1", temperature=0.3)

# 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ (Retrieval) - ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡πÅ‡∏•‡πâ‡∏ß
def retrieve_data(query_text):
    print(f"   üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á: {query_text}...")
    query_vector = embeddings.embed_query(query_text)
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Function v2 ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Å‡πâ Error ‡πÑ‡∏ß‡πâ
    params = {
        "query_embedding": query_vector,
        "match_threshold": 0.4, # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        "match_count": 5        # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏±‡∏Å 5 ‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏µ
    }
    response = supabase.rpc("match_sections_v2", params).execute()
    return response.data

# 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (Generation)
def generate_answer(question):
    # 4.1 ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
    retrieved_docs = retrieve_data(question)
    
    if not retrieved_docs:
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

    # 4.2 ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏õ‡πá‡∏ô Text ‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Context)
    context_text = ""
    for doc in retrieved_docs:
        sec_num = doc.get('section_number', '?')
        text = doc.get('text_original', '')
        context_text += f"- ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {sec_num}: {text}\n\n"

    print("   ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")

    # 4.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt (‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á)
    template = """
    ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢" ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô 
    
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢:
    {context}
    
    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {question}
    
    ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
    1. ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
    2. ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡πÄ‡∏ä‡πà‡∏ô "‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏≤ 32...")
    3. ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ ‡∏ß‡πà‡∏≤ "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠" ‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏≠‡∏á
    
    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
    """
    
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])
    
    # 4.4 ‡∏™‡∏£‡πâ‡∏≤‡∏á Chain ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô
    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": context_text, "question": question})
    
    return answer

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö ---
if __name__ == "__main__":
    print("\n‚öñÔ∏è  ‡∏ó‡∏ô‡∏≤‡∏¢ AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö!")
    print("-----------------------------------")
    
    while True:
        user_input = input("\nüó£Ô∏è  ‡∏ñ‡∏≤‡∏°‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö (‡∏û‡∏¥‡∏°‡∏û‡πå '‡∏≠‡∏≠‡∏Å' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏ö): ")
        if user_input.strip() in ['‡∏≠‡∏≠‡∏Å', 'exit', 'quit']:
            print("üëã ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ñ‡∏£‡∏±‡∏ö")
            break
            
        if user_input.strip():
            final_answer = generate_answer(user_input)
            print(f"\nüí° ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n{final_answer}")
            print("-" * 50)